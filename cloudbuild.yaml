substitutions:
  _STAGING_BUCKET: us-staging-pso-wmt-dp-spawner
  _CLUSTER_NAME: pso-wmt-dp-cluster
  _DATAPROC_STAGING: olopa-dp-staging
  _REGION: us-central1
  _ZONE: us-central1-a
  _SUBNET: olopa-network
  _MASTER_NODES_NUM: '1'
  _WORKER_NODES_NUM: '2'
  _MASTER_MACHINE_TYPE: n1-standard-4
  _WORKER_MACHINE_TYPE: n1-standard-4
  _MASTER_BOOT_DISK_SIZE: '500'
  _WORKER_BOOT_DISK_SIZE: '500'
  _INIT_SCRIPT: gs://us-staging-pso-wmt-dp-spawner/init-actions/dr-elephant.sh
  _PROJECT: pso-wmt-dp-spawner
steps:
# Build Dr. Elephant
- name: 'google/cloud-sdk:285.0.1'
  id: 'Build Dr. Elephant'
  args:
  - 'bash'
  - './build.sh'
  env:
  - 'GIT_TAG=$TAG_NAME'
# Upload zip archive to GCS
- name: 'gcr.io/cloud-builders/gsutil'
  id: 'Upload archive'
  args:
  - 'cp'
  - '/workspace/dist/dr-elephant-*.zip'
  - 'gs://${_STAGING_BUCKET}/artifacts/'
# Upload init-action to GCS
- name: 'gcr.io/cloud-builders/gsutil'
  id: 'Upload init-action'
  args:
  - 'cp'
  - '-r'
  - '/workspace/scripts/init-actions/*.sh'
  - 'gs://${_STAGING_BUCKET}/init-actions/'
# Create Dataproc cluster
- name: 'gcr.io/cloud-builders/gcloud'
  id: 'Create Dataproc cluster'
  args:
  - 'dataproc'
  - 'clusters'
  - 'create'
  - '${_CLUSTER_NAME}'
  - '--bucket=${_DATAPROC_STAGING}'
  - '--region=${_REGION}'
  - '--subnet=${_SUBNET}'
  - '--zone=${_ZONE}'
  - '--master-machine-type=${_MASTER_MACHINE_TYPE}'
  - '--master-boot-disk-size=${_MASTER_BOOT_DISK_SIZE}'
  - '--num-workers=${_WORKER_NODES_NUM}'
  - '--worker-machine-type=${_WORKER_MACHINE_TYPE}'
  - '--worker-boot-disk-size=${_WORKER_BOOT_DISK_SIZE}'
  - '--image-version=1.4-debian9'
  - '--labels=principal=olopa'
  - '--tags=olopa'
  - '--project=${_PROJECT}'
  - '--scopes=cloud-platform'
  - '--initialization-actions=${_INIT_SCRIPT}'
  - '--properties=dataproc:yarn.log-aggregation.enabled=true,yarn:yarn.nodemanager.remote-app-log-dir=gs://${_DATAPROC_STAGING}/log-aggregation,spark:spark.jars.packages=qubole:sparklens:0.3.0-s_2.11,spark:spark.extraListeners=com.qubole.sparklens.QuboleJobListener,spark:spark.sparklens.reporting.disabled=false,spark:spark.sparklens.data.dir=gs://olopa-dp-staging/sparklens'
# Run Hadoop MapReduce PI calculation test
- name: 'gcr.io/cloud-builders/gcloud'
  id: 'Run Hadoop MapReduce PI'
  args:
  - 'dataproc'
  - 'jobs'
  - 'submit'
  - 'hadoop'
  - '--cluster=${_CLUSTER_NAME}'
  - '--region=${_REGION}'
  - '--project=${_PROJECT}'
  - '--jar=file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
  - '--'
  - 'pi'
  - '-D'
  - 'mapreduce.map.memory.mb=4096'
  - '-D'
  - 'mapreduce.reduce.memory.mb=4096'
  - '100'
  - '2000000'
# Run Spark PI calculation test
- name: 'gcr.io/cloud-builders/gcloud'
  id: 'Run Spark PI'
  args:
  - 'dataproc'
  - 'jobs'
  - 'submit'
  - 'spark'
  - '--cluster=${_CLUSTER_NAME}'
  - '--region=${_REGION}'
  - '--project=${_PROJECT}'
  - '--jars=file:///usr/lib/spark/examples/jars/spark-examples.jar'
  - '--class=org.apache.spark.examples.SparkPi'
  - '--'
  - '1000'
# Delete Dataproc cluster
# - name: 'gcr.io/cloud-builders/gcloud'
#   id: 'Delete Dataproc cluster'
#   args:
#   - 'dataproc'
#   - 'clusters'
#   - 'delete'
#   - '${_CLUSTER_NAME}'
#   - '--region=${_REGION}'
#   - '--zone=${_ZONE}'
timeout: 1800s
